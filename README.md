# DL-Assignment-2
1. Translation
For this sequence-to-sequence model built using LSTM layers, we assume an input embedding size ğ‘š=256m=256, hidden state size ğ‘˜=256k=256 for both the encoder and decoder, sequence length ğ‘‡=15T=15, and vocabulary size ğ‘‰=60V=60. The model does not use attention and relies on greedy decoding during inference. The total number of computations is calculated using the formula ğ‘‡Ã—[8ğ‘˜(ğ‘š+ğ‘˜)+ğ‘˜ğ‘‰]TÃ—[8k(m+k)+kV]. Substituting the values gives 
 15Ã—[8Ã—256Ã—(256+256)+256Ã—60]=15Ã—(1048576+15360)=15Ã—1063936=15,959,04015Ã—[8Ã—256Ã—(256+256)+256Ã—60]=15Ã—(1048576+15360)=15Ã—1063936=15,959,040. Hence, the total number of computations is approximately 15.96 million operations.

The total number of parameters in the model is computed using the formula 2ğ‘‰ğ‘š+8ğ‘˜(ğ‘š+ğ‘˜+1)+ğ‘˜ğ‘‰+ğ‘‰2Vm+8k(m+k+1)+kV+V. Plugging in the values, we get 2Ã—60Ã—256+8Ã—256Ã—(256+256+1)+256Ã—60+60=30,720+1,050,624+15,360+60=1,096,7642Ã—60Ã—256+8Ã—256Ã—(256+256+1)+256Ã—60+60=30,720+1,050,624+15,360+60=1,096,764. Therefore, the total number of parameters in the network is approximately 1.10 million.
