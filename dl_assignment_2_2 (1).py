# -*- coding: utf-8 -*-
"""DL Assignment 2.2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCVY11WMhB_hYMMSOWb_0fiasJoSF1ny
"""

!pip install datasets

import os
os.environ["WANDB_DISABLED"] = "true"

import pandas as pd
from datasets import Dataset
from transformers import (
    GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments,
    DataCollatorForLanguageModeling, pipeline
)
import torch

# Load and clean the lyrics dataset
df = pd.read_excel("./Billie_Eilish_Lyrics.xlsx")

# Adjust this column name if it's different
df = df.dropna(subset=["Lyrics"])
df = df[df["Lyrics"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]
lyrics_data = df["Lyrics"].tolist()

# Create Hugging Face Dataset
dataset = Dataset.from_dict({"text": lyrics_data})

# Load pre-trained GPT-2 model and tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Tokenize the lyrics
def tokenize_batch(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True, max_length=128)

tokenized_dataset = dataset.map(tokenize_batch, batched=True)

# Prepare data collator for causal language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-billie-finetuned",
    overwrite_output_dir=True,
    per_device_train_batch_size=2,
    num_train_epochs=3,
    logging_steps=50,
    save_steps=200,
    save_total_limit=1,
    prediction_loss_only=True,
    fp16=torch.cuda.is_available()
)

# Setup Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator
)

# Train the model
trainer.train()

# Save model and tokenizer
trainer.save_model("./gpt2-billie-finetuned")
tokenizer.save_pretrained("./gpt2-billie-finetuned")

# Load fine-tuned model for text generation
generator = pipeline("text-generation", model="./gpt2-billie-finetuned", tokenizer=tokenizer)

# Example prompts for lyrics generation
sample_prompts = [
    "When the party's over",
    "I'm the bad guy",
    "I had a dream I got everything I wanted",
    "You should see me in a crown",
    "What do you want from me?"
]

# Generate and print lyrics for each prompt
for prompt in sample_prompts:
    print(f"\nPrompt: {prompt}")
    result = generator(prompt, max_length=100, num_return_sequences=1)[0]["generated_text"]
    print("Generated Lyrics:\n" + result)